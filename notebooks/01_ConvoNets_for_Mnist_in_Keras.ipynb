{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvoNets for Mnist in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments.[5] Furthermore, the black and white images from NIST were normalized to fit into a 20x20 pixel bounding box and anti-aliased, which introduced grayscale levels.\"\n",
    "\n",
    "https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mnist_digits](img/mnist.jpeg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not already installed, keras do so before proceeding by using pip:\n",
    "\n",
    "```pip install keras```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.datasets.mnist import load_data\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the mnist example provided with the Keras code base.\n",
    "\n",
    "_Batch size_ defines number of samples that going to be propagated through the network.  Batching helps the network to train faster and to use less memory.\n",
    "\n",
    "_Classes_ are the categories that our image data falls into; in this case, there are 10 digits.\n",
    "\n",
    "_Epoches_ are the number of training iterations a network goes through.  In general, model accuracy improves with more epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 164s - loss: 0.3283 - acc: 0.9010 - val_loss: 0.0839 - val_acc: 0.9729\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 182s - loss: 0.1152 - acc: 0.9651 - val_loss: 0.0559 - val_acc: 0.9815\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 187s - loss: 0.0860 - acc: 0.9749 - val_loss: 0.0453 - val_acc: 0.9847\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 213s - loss: 0.0738 - acc: 0.9777 - val_loss: 0.0431 - val_acc: 0.9857\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 197s - loss: 0.0649 - acc: 0.9809 - val_loss: 0.0367 - val_acc: 0.9876\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 169s - loss: 0.0559 - acc: 0.9831 - val_loss: 0.0321 - val_acc: 0.9893\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 173s - loss: 0.0501 - acc: 0.9849 - val_loss: 0.0305 - val_acc: 0.9890\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 188s - loss: 0.0480 - acc: 0.9857 - val_loss: 0.0326 - val_acc: 0.9891\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 191s - loss: 0.0450 - acc: 0.9863 - val_loss: 0.0293 - val_acc: 0.9898\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 177s - loss: 0.0412 - acc: 0.9876 - val_loss: 0.0287 - val_acc: 0.9905\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 179s - loss: 0.0403 - acc: 0.9881 - val_loss: 0.0304 - val_acc: 0.9902\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 184s - loss: 0.0358 - acc: 0.9889 - val_loss: 0.0282 - val_acc: 0.9906\n",
      "Test loss: 0.0281902231486\n",
      "Test accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
